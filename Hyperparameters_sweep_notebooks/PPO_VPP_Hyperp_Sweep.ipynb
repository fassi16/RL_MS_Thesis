{"cells":[{"cell_type":"markdown","metadata":{"id":"g7NT2ScfCgZp"},"source":["Installing required packages and dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_SYbpdxyUWs"},"outputs":[],"source":["%%capture\n","!pip install py-elvis\n","!pip install pyyaml==5.4\n","!pip install plotly==5.9.0\n","!pip install -U kaleido\n","\n","!pip install stable-baselines3[extra]\n","!pip install stable-baselines\n","!pip install sb3-contrib\n","!pip install gym\n","!pip install -q wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1jW0iIkMXps"},"outputs":[],"source":["#Cloning repository and changing directory\n","!git clone https://github.com/francescomaldonato/RL_VPP_Thesis.git\n","%cd RL_VPP_Thesis/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKRmoM-ITuh_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663797684098,"user_tz":-120,"elapsed":6122,"user":{"displayName":"Franz","userId":"00104379049391791155"}},"outputId":"a6655fe9-5688-46fa-b70e-a5bbe0983500"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch-Cuda available device: True\n","OS: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic #1 SMP Fri Aug 26 08:44:51 UTC 2022\n","Python: 3.7.14\n","Stable-Baselines3: 1.6.0\n","PyTorch: 1.12.1+cu113\n","GPU Enabled: True\n","Numpy: 1.21.6\n","Gym: 0.21.0\n","\n","({'OS': 'Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic #1 SMP Fri Aug 26 08:44:51 UTC 2022', 'Python': '3.7.14', 'Stable-Baselines3': '1.6.0', 'PyTorch': '1.12.1+cu113', 'GPU Enabled': 'True', 'Numpy': '1.21.6', 'Gym': '0.21.0'}, 'OS: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic #1 SMP Fri Aug 26 08:44:51 UTC 2022\\nPython: 3.7.14\\nStable-Baselines3: 1.6.0\\nPyTorch: 1.12.1+cu113\\nGPU Enabled: True\\nNumpy: 1.21.6\\nGym: 0.21.0\\n')\n","wandb, version 0.13.3\n"]}],"source":["import yaml\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","from gym import Env\n","from VPP_environment import VPPEnv, VPP_Scenario_config\n","from elvis.config import ScenarioConfig\n","import os\n","import wandb\n","from wandb.integration.sb3 import WandbCallback\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3 import PPO #The available algoritmhs in sb3-contrib for the custom environment with MultiInputPolicy\n","from sb3_contrib.common.maskable.utils import get_action_masks\n","import stable_baselines3 as sb3\n","from stable_baselines3.common.env_checker import check_env\n","import random\n","\n","#Check if cuda device is available for training\n","print(\"Torch-Cuda available device:\", torch.cuda.is_available())\n","print(sb3.get_system_info())\n","!wandb --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLqDVxThY6fQ"},"outputs":[],"source":["# Ensure deterministic behavior\n","torch.backends.cudnn.deterministic = True\n","random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hh5hkFi8SLTA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663797684546,"user_tz":-120,"elapsed":466,"user":{"displayName":"Franz","userId":"00104379049391791155"}},"outputId":"b01e0802-1242-4acc-a869-c6246e40ee3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vehicle types: <generator object ScenarioConfig.__str__.<locals>.<genexpr> at 0x7f3fdb498b50>Mean parking time: 18\n","Std deviation of parking time: 4\n","Mean value of the SOC distribution: 0.5\n","Std deviation of the SOC distribution: 0.2\n","Max parking time: 24\n","Number of charging events per week: 35\n","Vehicles are disconnected only depending on their parking time\n","Queue length: 0\n","Opening hours: None\n","Scheduling policy: Uncontrolled\n","\n","{'start_date': '2022-01-01T00:00:00', 'end_date': '2023-01-01T00:00:00', 'resolution': '0:15:00', 'num_households': 4, 'solar_power': 16, 'wind_power': 12, 'charging_stations_n': 6, 'EVs_n_max': 1827, 'EV_load_max': 66, 'houseRWload_max': 10, 'av_max_energy_price': 0.13}\n"]}],"source":["#Loading paths for input data\n","current_folder = ''\n","VPP_data_input_path = current_folder + 'data/data_testing/environment_table/' + 'Environment_data_2020.csv'\n","elvis_input_folder = current_folder + 'data/config_builder/'\n","\n","case = 'wohnblock_household_simulation_adaptive.yaml'\n","with open(elvis_input_folder + case, 'r') as file:\n","    yaml_str = yaml.full_load(file)\n","\n","elvis_config_file = ScenarioConfig.from_yaml(yaml_str)\n","VPP_config_file = VPP_Scenario_config(yaml_str)\n","\n","print(elvis_config_file)\n","print(VPP_config_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KP5gtqsXSP7r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663797688326,"user_tz":-120,"elapsed":3785,"user":{"displayName":"Franz","userId":"00104379049391791155"}},"outputId":"f0ee0502-06ca-4233-a66d-e56bef0c6614"},"outputs":[{"output_type":"stream","name":"stdout","text":["Charging event: 1, Arrival time: 2022-01-01 04:30:00, Parking_time: 17.147998909376643, Leaving_time: 2022-01-01 21:38:52.796074, SOC: 0.5863418486035888, SOC target: 1.0, Connected car: Tesla, Model S \n"," ... \n"," Charging event: 1825, Arrival time: 2022-12-31 16:00:00, Parking_time: 15.201427380293573, Leaving_time: 2023-01-01 07:12:05.138569, SOC: 0.47611396479812984, SOC target: 1.0, Connected car: Tesla, Model S \n","\n","ELVIS simulation: Tot_energy_consumed=kWh  60750.3821053751 , Av.load=kW  6.9347772158757 , Std.load=kW  12.742218960579994 , Total_cost=€  2135.55656989922 , Av.EV_load=kW  9.356470156131401 , Charging_events=  1825\n"]}],"source":["#Environment initialization\n","env = VPPEnv(VPP_data_input_path, elvis_config_file, VPP_config_file)\n","env.plot_ELVIS_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rI_79t9_8gXw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663797688769,"user_tz":-120,"elapsed":463,"user":{"displayName":"Franz","userId":"00104379049391791155"}},"outputId":"a232aff9-699f-48a6-d181-51b3eed14a76"},"outputs":[{"output_type":"stream","name":"stdout","text":["- ELVIS.Simulation:\n"," Energy_consumed=kWh  61218.63 , Av.load=kW  6.99 , Std.load=kW  12.58 , Total_cost=€  2118.93 , Av.EV_load=kW  9.41 , Av.EV_en_left=kWh  100.0 , Charging_events=  1825\n","Simulating VPP....\n"]}],"source":["#Function to check custom environment and output additional warnings if needed\n","check_env(env)\n","env.plot_reward_functions()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIpvXHb748le"},"outputs":[],"source":["PPO_path = \"trained_models/PPO_models/PPO\"\n","\n","#In Colab, uncomment below:\n","%env \"WANDB_DISABLE_CODE\" True\n","%env \"WANDB_NOTEBOOK_NAME\" \"Hyperparameters_sweep_notebooks/PPO_VPP_Hyperp_Sweep.ipynb\"\n","os.environ['WANDB_NOTEBOOK_NAME'] = 'Hyperparameters_sweep_notebooks/PPO_VPP_Hyperp_Sweep.ipynb'\n","wandb.login(relogin=True)\n","\n","#In local notebook, uncomment below:\n","#your_wandb_login_code = 0123456789abcdefghijklmnopqrstwxyzàèìòù0 #example length\n","#!wandb login {your_wandb_login_code}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WWpePfSSdJ_"},"outputs":[],"source":["sweep_config = {\n","    \"name\": \"PPO-sweep\",\n","    \"project\": \"RL_VPP_Thesis\",\n","    #\"entity\" : \"user_avocado\",\n","\n","    \"method\" : \"random\",\n","    \"metric\": {\"name\": \"cumulative_reward\",\n","                #\"target\": 10000000000,\n","                \"goal\": \"maximize\"},\n","    \"early_terminate\":{    \n","        \"type\": \"hyperband\",\n","        \"min_iter\": 10,\n","        \"eta\": 3\n","        },\n","\n","    \"parameters\": {\n","        \"policy_type\": {\n","            \"value\": \"MultiInputPolicy\"\n","            },\n","        \"epochs\": {\n","            \"value\": 1,\n","            },\n","        \n","        # \"n_steps\": {\n","        #     \"values\" : [2920, 5840, 11684]\n","        #     },\n","        \"batch_size\": {\n","            #\"value\" : 2920\n","            \"values\" : [2920, 8760, 11680, 17520]\n","            },\n","        \"n_epochs\": {\n","            \"value\" : 15,\n","            # \"min\": 10,\n","            # \"max\": 20,\n","            # \"distribution\": \"int_uniform\"\n","            },\n","        \"total_timesteps\": {\n","            \"value\": 500000,\n","            #\"min\": 500000,\n","            #\"max\": 1500000,\n","            #\"distribution\": \"int_uniform\"\n","            },\n","        \"learning_rate\": {\n","            \"min\": 0.0001,\n","            \"max\": 0.0014,\n","            \"distribution\": \"uniform\"\n","            },\n","        \"lr_schedule\": {\n","            \"value\": \"constant\",\n","            #\"values\": [\"linear\", \"constant\"]\n","            },\n","        \"gamma\": { #discount factor\n","            \"max\": 0.9999,\n","            \"min\": 0.89,\n","            \"distribution\": \"log_uniform_values\"\n","            },\n","        \"gae_lambda\": {\n","            \"values\": [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0]\n","            },\n","        \"clip_range\":{\n","            \"values\": [0.1, 0.2, 0.3, 0.4]\n","            },\n","        \"ent_coef\": { #entropy coefficient while calculating the loss\n","            \"min\": 1e-10,\n","            \"max\": 0.1,\n","            \"distribution\": \"log_uniform_values\"\n","            },\n","        \"vf_coef\":{\n","            \"min\": 0.0,\n","            \"max\": 1.0,\n","            \"distribution\": \"uniform\"\n","            },\n","        \"normalize_advantage\":{\n","            #\"values\":  [True, False]\n","            \"value\": True\n","            },\n","        \"max_grad_norm\": {\n","            \"values\": [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5]\n","            },\n","\n","        \"ortho_init\": {\n","            #\"values\": [True, False],\n","            \"value\": True,\n","            },\n","        \"net_arch\": {\n","            #\"values\": [\"small-short\", \"small-long\", \"medium-short\", \"medium\", \"big-short\", \"big\", \"big-large\"],\n","            \"values\": [\"small-separate\", \"small-shared\", \"medium-short\", \"medium\", \"medium-separate\", \"big-shared\", \"big\", \"big-separate\"]\n","            },\n","        \"activation_fn\":{\n","            \"value\": \"Tanh\",\n","            #\"values\": [\"Tanh\", \"ReLU\"],\n","            #\"values\": [\"Tanh\", \"ReLU\", \"ELU\"],\n","            #\"values\": [\"Tanh\", \"ReLU\", \"ELU\", \"LeakyReLU\"]\n","            },\n","        \"optimizer_class\": {\n","            \"value\": \"RMSprop\",\n","            #\"values\": [\"RMSprop\", \"Adam\", \"SGD\"]\n","            }\n","    }\n","}\n","\n","policy_dict = {\n","    \"net_arch\": {\n","         \"small-separate\": [dict(pi=[64, 64, 64], vf=[64, 64, 64])],\n","        \"small-shared\": [128, dict(pi=[64, 64], vf=[64, 64])],\n","        \n","        \"medium-short\": [dict(pi=[128, 128], vf=[128, 128])],\n","        \"medium\": [64, dict(pi=[256, 256], vf=[256, 256])],\n","        \"medium-separate\": [dict(pi=[256, 256], vf=[256, 256])],\n","\n","        \"big-shared\": [256, dict(pi=[64, 64], vf=[64, 64])],\n","        \"big\": [128, dict(pi=[512, 512], vf=[512, 512])],\n","        \"big-separate\": [dict(pi=[512, 512], vf=[512, 512])],\n","        },\n","        \n","    \"activation_fn\":{\n","        \"Tanh\": torch.nn.modules.activation.Tanh,\n","        \"ReLU\": torch.nn.modules.activation.ReLU,\n","        \"ELU\": torch.nn.modules.activation.ELU,\n","        \"LeakyReLU\": torch.nn.modules.activation.LeakyReLU\n","        },\n","\n","    \"optimizer_class\": {\n","        \"RMSprop\": torch.optim.RMSprop,\n","        \"Adam\": torch.optim.Adam,\n","        \"SGD\": torch.optim.SGD\n","        }\n","}\n","\n","#to enable adjustable learning rate\n","from typing import Callable, Union\n","def linear_schedule(initial_value: Union[float, str]) -> Callable[[float], float]:\n","    if isinstance(initial_value, str):\n","        initial_value = float(initial_value)\n","    def func(progress_remaining: float) -> float:\n","        return progress_remaining * initial_value\n","    return func\n","\n","#Trained model testing function\n","def evaluate_model(env, model, wandb_run_dir, deterministic=True):\n","    obs = env.reset()\n","    done = False\n","    cumulative_reward = 0\n","    # cell and hidden state of the LSTM\n","    lstm_states = None\n","    num_envs = 1\n","    # Episode start signals are used to reset the lstm states\n","    episode_starts = [True]\n","    while not done:\n","        # Retrieve current action mask\n","        action_masks = get_action_masks(env)\n","        action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic = deterministic) #Now using our trained model with deterministic prediction [should improve performances]\n","        #env.lstm_state = lstm_states\n","        obs, reward, done, info = env.step(action)\n","        episode_starts = done\n","        cumulative_reward+=reward\n","    VPP_results_plot = env.plot_VPP_results()\n","    VPP_results_plot.show()\n","    VPP_results_plot.write_image(os.path.join(wandb_run_dir, \"VPP_results_plot.png\"))\n","    plot_VPP_energies = env.plot_VPP_energies()\n","    plot_VPP_energies.show()\n","    plot_VPP_energies.write_image(os.path.join(wandb_run_dir, \"VPP_energies_plot.png\"))\n","    plot_rewards_results = env.plot_rewards_results()\n","    plot_rewards_results.show()\n","    plot_rewards_results.write_image(os.path.join(wandb_run_dir, \"VPP_rewards_plot.png\"))\n","    plot_rewards_stats = env.plot_rewards_stats()\n","    plot_rewards_stats.show()\n","    plot_rewards_stats.write_image(os.path.join(wandb_run_dir, \"VPP_rewards_stats_plot.png\"))\n","    plot_EVs_kpi = env.plot_EVs_kpi()\n","    plot_EVs_kpi.show()\n","    plot_EVs_kpi.write_image(os.path.join(wandb_run_dir, \"EVs_kpi_plot.png\"))\n","    plot_actions_kpi = env.plot_actions_kpi()\n","    plot_actions_kpi.show()\n","    plot_actions_kpi.write_image(os.path.join(wandb_run_dir, \"actions_kpi_plot.png\"))\n","    plot_load_kpi = env.plot_load_kpi()\n","    plot_load_kpi.show()\n","    plot_load_kpi.write_image(os.path.join(wandb_run_dir, \"load_kpi_plot.png\"))\n","    plot_yearly_log_kpi = env.plot_yearly_load_log()\n","    plot_yearly_log_kpi.show()\n","    plot_yearly_log_kpi.write_image(os.path.join(wandb_run_dir, \"yearly_log_load_plot.png\"))\n","    plot_comparison = env.plot_VPP_Elvis_comparison()\n","    plot_comparison.show()\n","    plot_comparison.write_image(os.path.join(wandb_run_dir, \"Elvis_VPP_comparison_plot.png\"))\n","    VPP_table = env.save_VPP_table(save_path=os.path.join(wandb_run_dir,'VPP_table.csv'))\n","    #wandb.log({\"VPP_table\": VPP_table})\n","\n","    wandb.save(f\"VPP_results_plot.png\")\n","    wandb.save(f\"VPP_energies_plot.png\")\n","    wandb.save(f\"VPP_rewards_plot.png\")\n","    wandb.save(f\"VPP_rewards_stats_plot.png\")\n","    wandb.save(f\"EVs_kpi_plot.png\")\n","    wandb.save(f\"actions_kpi_plot.png\")\n","    wandb.save(f\"load_kpi_plot.png\")\n","    wandb.save(f\"yearly_log_load_plot.png\")\n","    wandb.save(f\"Elvis_VPP_comparison_plot.png\")\n","    wandb.save(f\"VPP_table.csv\")\n","    # wandb.log({\"VPP_results_plot\": wandb.Image(\"VPP_results_plot.png\")})\n","    return cumulative_reward, env.av_EV_energy_left, env.overconsumed_en, env.underconsumed_en, env.sim_overcost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2s-8sdfMSdKB"},"outputs":[],"source":["#ENVIRONMENT WRAPPING\n","X_env = Monitor(env)\n","#Vectorized environment wrapper\n","X_env = DummyVecEnv([lambda: X_env])\n","#Sync custom tensorboard patch\n","tensorboard_log_path = \"wandb/tensorboard_log/\"\n","#wandb.tensorboard.patch(root_logdir = tensorboard_log_path, pytorch=True)\n","\n","#Model training function for Hyperparameters Sweep\n","def train_func():\n","    with wandb.init(#job_type='PPO_trained_model',\n","                    reinit=True,\n","                    #settings=wandb.Settings(symlink=False),\n","                    sync_tensorboard=True,\n","                    monitor_gym=False, save_code=False) as run:\n","        writer = SummaryWriter(tensorboard_log_path)\n","        config = wandb.config\n","        policy_kwargs =  dict(\n","            ortho_init =config[\"ortho_init\"],\n","            net_arch = policy_dict[\"net_arch\"][config[\"net_arch\"]],\n","            activation_fn = policy_dict[\"activation_fn\"][config[\"activation_fn\"]],\n","            optimizer_class = policy_dict[\"optimizer_class\"][config[\"optimizer_class\"]]\n","        )\n","        # if config[\"batch_size\"] > config[\"n_steps\"]:\n","        #     batch_size = config[\"n_steps\"]\n","        # else: batch_size = config[\"batch_size\"]\n","        if config[\"lr_schedule\"] == \"linear\":\n","            learning_rate = linear_schedule(config[\"learning_rate\"])\n","        elif config[\"lr_schedule\"] == \"constant\": learning_rate = config[\"learning_rate\"]\n","\n","        #model definition\n","        model = PPO(config[\"policy_type\"], X_env,\n","                    learning_rate = learning_rate,\n","                    n_steps = config[\"batch_size\"],\n","                    batch_size = config[\"batch_size\"],\n","                    n_epochs = config[\"n_epochs\"],\n","                    gamma = config[\"gamma\"],\n","                    gae_lambda = config[\"gae_lambda\"],\n","                    clip_range = config[\"clip_range\"],\n","                    ent_coef = config[\"ent_coef\"],\n","                    vf_coef = config[\"vf_coef\"],\n","                    normalize_advantage = config[\"normalize_advantage\"],\n","                    max_grad_norm = config[\"max_grad_norm\"],\n","                    #create_eval_env = False,\n","                    policy_kwargs = policy_kwargs,\n","                    verbose=1,\n","                    tensorboard_log= tensorboard_log_path\n","                    #tensorboard_log= wandb.run.dir\n","                    )\n","        \n","        #Training of each hyperprameter set    \n","        for epoch in range(config[\"epochs\"]):\n","            model.learn(total_timesteps=config[\"total_timesteps\"],\n","                tb_log_name=f'PPO_{run.id}',\n","                callback=WandbCallback(\n","                    gradient_save_freq=1000,\n","                    #model_save_path=f\"trained_models/PPO_sweep_{run.id}\",\n","                    verbose=1)\n","                )\n","            ##TESTING trained model\n","            #cumulative_reward, std_reward = evaluate_policy(model, X_env, n_eval_episodes=1, render=False)\n","            cumulative_reward, av_EV_energy_left, std_EV_energy_left, total_load, av_load, std_load, total_cost = evaluate_model(env, model, wandb.run.dir, deterministic=True)\n","            ##Logging results\n","            wandb.log({\"cumulative_reward\":cumulative_reward, \"av_EV_energy_left\":av_EV_energy_left, \"std_EV_energy_left\":std_EV_energy_left, \"total_load\":total_load, \"av_load\":av_load, \"std_load\":std_load, \"total_cost\":total_cost})\n","            #model.save(current_folder + PPO_path + f\"_{run.id}\")\n","            model.save(os.path.join(wandb.run.dir, f\"model_MaskablePPO.{run.id}\"))\n","            wandb.save(f\"model_MaskablePPO.{run.id}\")  \n","        # Sync wandb\n","        #wandb.save(glob.glob(f\"runs/*.pt.trace.json\")[0], base_path=f\"runs\")\n","        #wandb.save(f'wandb/tensorboard_log/MaskablePPO_{run.id}')\n","        run.finish()\n","            \n","        #print(f\"END OF TRAINING #### Model reward: {cumulative_reward}; Training complete.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uhFNXR0v3PP"},"outputs":[],"source":["count = 40 # number of Training runs to execute\n","\n","sweep_id = wandb.sweep(sweep_config)\n","wandb.agent(sweep_id, train_func, count=count)\n","\n","#!wandb sync log_dir\n","!wandb sync wandb/tensorboard_log/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMf4k5g8KFE7"},"outputs":[],"source":["env.close()\n","#run.finish()\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}